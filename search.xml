<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
    
    <entry>
      <title><![CDATA[LIBGUESTFS分析介绍]]></title>
      <url>http://www.waverley.me/2017/04/14/LIBGUESTFS%E5%88%86%E6%9E%90%E4%BB%8B%E7%BB%8D/</url>
      <content type="text"><![CDATA[Libguestfs 是一套能够访问并修改虚机磁盘镜像的工具，可以用来查看编辑虚拟机磁盘内部文件，监控虚拟机的磁盘使用情况、创建虚机磁盘等。 1 概述Libguestfs——tools for accessing and modifying virtual machine disk images.Libguestfs 是一套能够访问并修改虚机磁盘镜像的工具，可以用来查看编辑虚拟机磁盘内部文件，修改虚拟机内的脚本、监控虚拟机的磁盘使用情况、创建虚机磁盘等待，libguestfs能够访问任何虚机磁盘，并且不需要root权限，且有多层次的防御攻击虚机磁盘镜像的功能；它还可以访问远程机器或者CDs/USB上的磁盘镜像等。Libguestfs从2009年持续开发到现在，有很丰富详细的使用手册，已经有很多应用成功的用户或项目，比如openstack。Libguestfs是一个C语言库，并且绑定十多种别的语言；开源KVM中的virt-builder、virt-install、virt-edit、virt-format、virt-inspector、virt-p2v、guestmount等一系列工具都是基于libguestfs库API来实现的。官方网址：http://libguestfs.org/开源代码：git clone git://github.com/libguestfs/libguestfs.git 2 openstack如何部署虚机的？2.1 openstack提供修改虚机磁盘镜像的方法openstack官网共提供了4种方法来修改虚机磁盘镜像：guestfish、guestmount、virt-*tools、nbd四种；其中前三种都是基于libguestfs库来实现的，第四种nbd正是CAS使用的方法。然而，openstack已经不推荐使用nbd了，而是推荐使用基于libguestfs库的方法，实际代码中也是这么做的；推荐的理由是：nbd直接将guest镜像挂载到host文件系统的方式是存在安全风险的，而libguestfs却可以提供至少两层防护； 2.2 nbd方式的安全隐患这里有一篇介绍nbd的安全隐患的文章：https://www.berrange.com/posts/2013/02/20/a-reminder-why-you-should-never-mount-guest-disk-images-on-the-host-os/当用nbd直接挂载文件系统时，VFS的一些漏洞可能被攻击者利用而成为一个恶意的文件系统；主要有两方面原因：1、 在内核中有很多文件系统驱动程序，但是他们有很多程序很少被使用，开发人员也很少去关注这些代码；linux用户空间可以帮助潜在的攻击者识别文件系统的类型，并且选择一些恶意的文件系统程序；2、 一个内核级漏洞就像一个本地root漏洞，可以给潜在的攻击者提供访问系统硬件的权限。 2.3 Libguestfs的安全防护Libguestfs可以通过创建虚机，在里面执行所有的文件系统操作的方式来阻止这种潜在的攻击；因此，即时攻击者利用VFS漏洞入侵到guest内核，他也必须突破虚机和secure virtualization后，才可能入侵到主机OS。Libguestfs提供了一个分层的方法来隔离入侵：http://libguestfs.org/guestfs-security.1.html untrusted filesystem -------------------------------------- appliance kernel -------------------------------------- qemu process running as non-root -------------------------------------- sVirt [if using libvirt + SELinux] -------------------------------------- host kernel 我们用libguestfs启动qemu虚机时，往往是作为non-root用户运行的。入侵者首先需要写一个文件系统入侵application kernel，然后入侵qemu虚拟化程序或者libguestfs协议，最后还要获取入侵host kernel的root权限。另外，如果再使用了libvirt和SELinux，sVirt 将进一步限制qemu进程。这样就大大增加了入侵的难度。我们用libguestfs启动qemu虚机时，往往是作为non-root用户运行的。入侵者首先需要写一个文件系统入侵application kernel，然后入侵qemu虚拟化程序或者libguestfs协议，最后还要获取入侵host kernel的root权限。另外，如果再使用了libvirt和SELinux，sVirt 将进一步限制qemu进程。这样就大大增加了入侵的难度。 3 Libguestfs实现原理3.1 Libguestfs组成Libguestfs主要有三大部分：guestfsd、guestfs-lib、guestfish。其中guestfsd是一个daemon，libguestfs是一个lib，guestfish是一个命令行工具。Guestfsd是一个daemon，但是它不是运行在host的daemon，而是运行在guest上，libguestfs首先用febootstrap和febootstrap-supermin-helper两个工具将host中的kernel，一些modules，配置文件和一些工具package重新组合到一起，然后在后台启动一个qemu进程读取这个有febootstrap工具链生成的image。在qemu启动的guest里运行guestfsd，guestfsd通过socket和host进行通信，之间建立了一个通信协议，它可以通过socket接受来自host端guestfs-lib写到socket的数据，guestfsd通过分析接收到的数据，进而执行相应的do*操作，do操作实际上是对guest端普通命令的一些封装，如果想实现一个NEW API，只要在guestfsd里用相应的do_对普通命令进行封装即可，然后将这个普通命令程序通过febootstrap打包到qemu启动时读取的image中。Guestfs-lib是一个库，它实现了一些libguestfs的库函数——guestfs_*。这些库函数向socket发送相应的数据，数据就会被guest端的guestfsd接收到，进而分析索要执行的操作。Guestfish是对guestfs-lib接口函数的一些应用，guestfish的命令都是通过调用guestfs-lib的库函数来实现的。因此，在使用libguestfs的时候，可以使用guestfish这样的命令行工具，也可以直接在程序（包括c，java，python等）中调用guestfs-lib实现的库函数。 3.2 Guestfish原理Guestfish –a vm.img 启动的进程，交互命令行是main program，当运行run时，会创建一个child process，在child process中，qemu运行一个成为appliance的小虚拟机。创建子进程是由guest_launch函数完成的。在appliance中，运行了linux kernel和一系列用户空间工具（LVM,ext2等），以及guestfsd。main process中的libguestfs和这个guestfsd通过RPC进行交互。由child process的kernel来操作vm.img。 以下为libguestfs启动的qemu进程： /usr/bin/kvm -global virtio-blk-pci.scsi=off -nodefconfig -enable-fips -nodefaults -display none -machine accel=kvm:tcg -cpu host -m 500 -no-reboot -rtc driftfix=slew -no-hpet -global kvm-pit.lost_tick_policy=discard -kernel /usr/local/lib/guestfs/appliance/kernel -initrd /usr/local/lib/guestfs/appliance/initrd -object rng-random,filename=/dev/urandom,id=rng0 -device virtio-rng-pci,rng=rng0 -device virtio-scsi-pci,id=scsi -drive file=/vms/images/win2003_r2_sp2_x64_base.img,cache=writeback,format=qcow2,id=hd0,if=none -device scsi-hd,drive=hd0 -drive file=/tmp/libguestfsdLLZTJ/overlay1,cache=unsafe,format=qcow2,id=hd1,if=none -device scsi-hd,drive=hd1 -drive file=/usr/local/lib/guestfs/appliance/root,snapshot=on,id=appliance,cache=unsafe,if=none,format=raw -device scsi-hd,drive=appliance -device virtio-serial-pci -serial stdio -chardev socket,path=/tmp/libguestfsIhONsg/guestfsd.sock,id=channel0 -device virtserialport,chardev=channel0,name=org.libguestfs.channel.0 -append panic=1 console=ttyS0 edd=off udevtimeout=6000 udev.event-timeout=6000 no_timer_check printk.time=1 cgroup_disable=memory usbcore.nousb cryptomgr.notests tsc=reliable 8250.nr_uarts=1 root=/dev/sdc selinux=0 quiet TERM=linux 3.3 并行处理Libguestfs应用是IO绑定的，可以并行运行多个appliances。假设有充足的内存，1个appliance和多个appliances之间的时间性能仅仅有细微的差别。在一个2核（4线程），16G内存的物理机上进行测试，下图展示了1个到20个appliances并行运行时良好的时间性能表现：从表中可以看出，大量部署虚机时，可以并行部署3台，与运行单个appliance的时间性能几乎是一样的，可以大大节约部署的时间。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[获取linux系统CPU实时利用率]]></title>
      <url>http://www.waverley.me/2017/04/02/%E8%8E%B7%E5%8F%96linux%E7%B3%BB%E7%BB%9FCPU%E5%AE%9E%E6%97%B6%E5%88%A9%E7%94%A8%E7%8E%87/</url>
      <content type="text"><![CDATA[在Linux系统中，CPU时间的分配信息保存在/proc/stat文件中，我们就可以从这个文件中读取原始数据，然后计算出最终的CPU利用率的。 在Linux系统中，CPU时间的分配信息保存在/proc/stat文件中，我们就可以从这个文件中读取原始数据，然后计算出最终的CPU利用率的。这个文件的一般格式如下所示： [root@localhost ~]# cat /proc/stat cpu 245501 20040 134027 122597803 39715 23822 74062 0 cpu0 96705 11277 84622 61249161 31399 23283 70496 0 cpu1 148795 8762 49405 61348642 8316 538 3566 0 intr 636408939 615480120 9 0 0 4 0 4 0 1 0 85 0 73 0 1844999 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 481644 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 16401031 0 0 0 0 0 0 0 2054028 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 146941 0 0 0 0 0 0 0 0 0 0 0 0 0 ctxt 30181583 btime 1488951210 processes 431551 procs_running 1 procs_blocked 0 文件的前几行记录了每个CPU核心的在各种状态下分配的时间片（单位是Jiffies，一般为0.01s），这些数据是从CPU加电到当前的累计值。常用的监控软件（例如top等）就是利用/proc/stat里面的这些数据计算CPU利用率的。 不同版本的linux /proc/stat文件内容有略微差异，各列参数说明如下： 参数 说明 User(1) 从系统启动开始累积到当前时刻，处于用户态的运行时间，不包含nice值为负进程； Nice(2) 从系统启动开始累积到当前时刻，nice值为负的进程所占用的CPU时间； System(3) 从系统启动开始累积到当前时刻，处于和心态的运行时间; Idle(4) 从系统启动开始累积到当前时刻，除IO等待时间外的其他等待时间； Iowait(5) 从系统启动开始累积到当前时刻，IO等待时间（since 2.5.41）; Irq(6) 从系统启动开始累积到当前时刻，硬中断时间（since 2.6.0-tests4）； Softirq(7) 从系统启动开始累积到当前时刻，软中断时间（since 2.6.0-test5）； Steal(8) Stolen time, which is the time spent in other operating systems when running in a virtualized environment(since 2.6.11); Guest(9) Time spent running a virtual CPU for guest operating systems under the control of the Linux kernel(since 2.6.24); guest_nice(10) Time spent running a niced guest (virtual CPU for guest operating systems under the control of the Linux kernel)( since 2.6.24); 因为/proc/stat中的数值都是从系统启动开始累积到当前时刻的累加值，所以需要在不同时间点t1和t2取值进行比较运算，当两个时间点的间隔较短时，就可以把这个计算结果看做是CPU的即时利用率。CPU即时利用率（时间间隔1s）计算公式如下：CPU在t1到t2时间段总的使用时间 = ( user2+ nice2+ system2+ idle2+ iowait2+ irq2+ softirq2) –( user1+ nice1+ system1+ idle1+ iowait1+ irq1+ softirq1)CPU在t1到t2时间段空闲使用时间 = (idle2 - idle1)CPU在t1到t2时间段即时利用率 = 1 - CPU空闲使用时间 / CPU总的使用时间 注：在利用公式进行CPU即时利用率计算时，一定要先判断Total和idle，有时计算出来的idle比Total还要大，出现时间倒流现象，较早的发行版OS内核可能会出现。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[如何查看Linux中进程运行在哪个CPU上]]></title>
      <url>http://www.waverley.me/2017/03/15/%E5%A6%82%E4%BD%95%E6%9F%A5%E7%9C%8BLinux%E4%B8%AD%E8%BF%9B%E7%A8%8B%E8%BF%90%E8%A1%8C%E5%9C%A8%E5%93%AA%E4%B8%AACPU%E4%B8%8A/</url>
      <content type="text"><![CDATA[当Linux进程运行在多核系统中时，我们如何才能找出这个进程到底是运行在哪个CPU上？ 当你在一个多核NUMA处理器上运行一个高性能的应用程序或者高负载网络程序时，CPU/memory是一个限制它们性能的重要因素。在相同的NUMA节点调度处理器可以减少访问远程memory的性能降低。Intel的Sandy Bridge处理器，具有一个集成的PCIe控制器，你可以利用NIC card的PCI-to-CPU亲和性在同一个NUMA节点上调度网络I/O。 作为性能优化和问题定位的一部分，我们有时需要知道具体进程是运行在哪个CPU核心或者NUMA节点的。 下面介绍几种方法，可以帮助我们找到一个Linux进程是运行在哪个CPU核心上的。 一、taskset如果一个进程被绑定在特定的CPU，可以使用taskset命令找到这个被绑定的CPU： 1$ taskset -c -p &lt;pid&gt; 例如，对于一个PID 5357的进程：12$ taskset -c -p 5357pid 5357&apos;s current affinity list: 5 命令输出显示这个进程是被绑定到CPU核心5了。然而，如果没有绑定任何CPU，将会看到如下输出：1pid 5357&apos;s current affinity list: 0-11 表明这个进程有可能被调度到0-11的任何CPU核心上，这种情况下，taskset将无法判断进程是被运行在哪个CPU上了，只能使用下面的方法。 二、psps命令能够显示出每个进程/线程被分配的CPU ID(“PSR”列)。123$ ps -o pid,psr,comm -p &lt;pid&gt; PID PSR COMMAND 5357 10 prog 这个输出表明PID 5357进程（名字为prog）现在是运行在CPU核心10上。如果这个进程未被绑定CPU，则PSR列会根据被分配到的CPU随时间变化的。 三、toptop命令也可以显示一个进程被分配的CPU。首先，运行top -p PID，然后按f键，并且增加“Last used CPU”列。现在正在使用的CPU核心将会显示在“P”（”PSR”）列下。1$ top -p 5357 相对ps命令，top命令的优势是你可以连续监视CPU如何随时间变化的。 四、htop还有一个查看进程/现场正在使用的CPU的方法是htop命令。运行htop命令，然后按键，选中”Columns”，在”Available Columns”下增加”PROCESSOR”。如下图”CPU”列就是进程被调用的CPU ID。 注意：之前的命令taskset/ps/top被分配CPU核心IDs从0到N-1，然而htop是从1到N。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[如何在C程序中嵌入python代码]]></title>
      <url>http://www.waverley.me/2017/03/04/%E5%A6%82%E4%BD%95%E5%9C%A8C%E7%A8%8B%E5%BA%8F%E4%B8%AD%E5%B5%8C%E5%85%A5python%E4%BB%A3%E7%A0%81/</url>
      <content type="text"><![CDATA[C语言与Python结合后，将会更加的强大。 python是目前最流行的语言之一，它具有简明的语法，易于学习、跨平台支持等特点。此外，还有很多高质量的python库和模块，使你用很少的代码就可以实现复杂的功能。这使得python成为最有效的开发工具之一。然而，python不如C语言快速，很多高性能软件例如Linux操作系统，web server和数据库都是用C语言写的。如果你正在用C开发程序，但是一部分代码需要用python来写，你就可以写一个模块，使用Python/C API把这个模块嵌入到C程序中. 下面让我们一起来用Python/C API把python代码嵌入到C程序中。 安装Python Development Package首先要安装Python Development Package，如下：Debian, Ubuntu or Linux Mint系统:1$ sudo apt-get install python2.7-dev CentOS, Fedora or RHEL系统:1sudo yum install python-devel 安装成功后，python头文件就在/usr/include/python2.7路径下，不同的Linux发行版，路径可能不同，例如CentOS6.x系统为/usr/include/python2.6。 初始化解释器并设置路径第一步是初始化python解释器，如下1Py_Initialize(); 初始化成功后，需要设置你要导入到C程序中的python模块路径。例如，我们的python模块位于路径 /usr/local/modules，然后调入如下C函数设置路径：1PySys_SetPath(&quot;/usr/local/modules&quot;); 数据转换将python嵌入到C中最重要的事情之一就是数据转换，为了把C数据传给python，我们需要将C数据类型转换为python数据类型。python/C API提供了一些函数，例如将C字符串转为python字符串，使用PyString_FromString()函数：123PyObject *pval;char *cString = &quot;Cyberpersons&quot;;pval = PyString_FromString(cString); PyInt_FromLong()函数将C的long类型转换为python的int类型，每个Python/C API函数都返回一个相应的PyObject类型。 定义一个python模块当你想把python代码嵌入到另外一种语言时，需要写一个python模块，然后”import”到其他语言中。例如，我们写一个简单的python模块。12def printData(data): return data+data+&apos;\n&apos; 上面的函数以字符串为参数，然后返回两个重复的字符串。这个模块的文件名称为”printData.py”然后把这个模块放入到之前声明的路径下（/usr/local/modules）。 载入python模块定了python模块，然后需要载入到C程序中。代码如下： 123// argv[1] specifies the module file name (&quot;printData.py&quot;).pName = PyString_FromString(argv[1]);pModule = PyImport_Import(pName); 构建函数参数载入模块后，就可以调用python模块中定义的函数了。一般我们需要传递一个或者多个参数到python函数中，我们就要构建一个包含python函数参数的python元组。在我们的例子中，printData()函数需要一个参数，因此我们构建的python元组包含一个元素。可以用PyTuple_SetItem()函数来进行元组的设置。 12345678PyObject *pythonArgument;pythonArgument = PyTuple_New(1);pValue = PyString_FromString(argv[3]); if(pValue==NULL)&#123; return 1;&#125;PyTuple_SetItem(pythonArgument, 0, pValue); 这样我们就成功构建了一个可以传给函数的参数，然后就可以在C程序中调用python函数了。 调用python函数一旦python元组被成功创建，我们就可以用这个参数调用python函数。因此，首先用PyObject_GetAttrString()函数获取到python函数的引用，然后调用函数PyObject_CallObject()，如下： 1234// argv[2] is the function name defined in pModule// pFunc is the reference to the functionpFunc = PyObject_GetAttrString(pModule, argv[2]);pValue = PyObject_CallObject(pFunc, pythonArgument); Error检查一个常用的消除运行时错误的方法是检查函数的返回值并且采取相应的动作。与C程序中一个error全局变量一样，当Python/C API函数失败，全局标识将会被设置到error，PyErr_Print()函数可以打印出可读调用栈，如下： 123456789pModule = PyImport_Import(pName);if (pModule != NULL) &#123; // Do something useful here&#125;else &#123; PyErr_Print(); // print traceback fprintf(stderr, &quot;Failed to load \&quot;%s\&quot;\n&quot;, argv[1]); return 1;&#125; 你可以在程序中很容易进行error检查。下面是一个完整的C程序，嵌入了python代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445// depending on distros, the exact path or Python version may vary.#include &lt;/usr/include/python2.7/Python.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;int main(int argc, char *argv[])&#123; PyObject *pName, *pModule, *pDict, *pFunc; PyObject *pArgs, *pValue; Py_Initialize(); PySys_SetPath(&quot;/usr/local/modules&quot;); // path to the module to import pName = PyString_FromString(argv[1]); pModule = PyImport_Import(pName); if (pModule != NULL) &#123; PyObject *pythonArgument; pythonArgument = PyTuple_New(1); pValue = PyString_FromString(argv[3]); if (pValue == NULL) &#123; return 1; &#125; PyTuple_SetItem(pythonArgument, 0, pValue); pFunc = PyObject_GetAttrString(pModule, argv[2]); if (pFunc &amp;&amp; PyCallable_Check(pFunc)) &#123; pValue = PyObject_CallObject(pFunc, pythonArgument); if (pValue != NULL) &#123; printf(&quot;Value returuend from the function %s&quot;, PyString_AsString(pValue)); &#125; else &#123; PyErr_Print(); &#125; &#125; else &#123; if (PyErr_Occurred()) PyErr_Print(); fprintf(stderr, &quot;Cannot find function \&quot;%s\&quot;\n&quot;, argv[2]); &#125; &#125; else &#123; PyErr_Print(); fprintf(stderr, &quot;Failed to load \&quot;%s\&quot;\n&quot;, argv[1]); return 1; &#125;&#125; 编译并运行将上面代码保存为finalCode.c，然后编译代码，用python库(-lpython2.7)连接。1$ gcc -o final finalCode.c -lpython2.7 然后用三个参数执行编译后的程序：1./final printData printData cyberpersons 这三个参数分别是模块名称、模块中的函数名称、传给python函数的字符串参数。最终，输出如下所示：]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[GIT:修改远端的提交]]></title>
      <url>http://www.waverley.me/2017/01/30/GIT-%E4%BF%AE%E6%94%B9%E8%BF%9C%E7%AB%AF%E7%9A%84%E6%8F%90%E4%BA%A4/</url>
      <content type="text"><![CDATA[当你不小心，写错了提交的commit/message，该如何处理呢？强大的GIT早已经为你备好了“后悔药”。 1、修改最后一次提交理论上，SCM是不应该修改历史记录的，提交的注释也，不过在git中，其commit提供了一个–amend参数，可以修改最后一次提交的信息。要对最后一次提交进行修改，如下：12git add xxxgit commit --ammend 这时出现的对话框中，可以进行message的编辑，然后退出，完成对本地最后一次提交的修改，然后再提交到远端：1git push origin remote_branch -f 2、修改历史提交git使用amend选项提供了最后一次commit的反悔。但是对于历史提交呢，就必须使用rebase了。1git rebase -i HEAD~3 表示要修改当前版本的倒数第三次状态。这个命令出来之后，会显示出类似如下：123pick:******* pick:******* pick:******* 如果你要修改哪个，就把那行的pick改成edit，然后退出。 这时通过git log你可以发现，git的最后一次提交已经变成你选的那个了，这时再使用步骤1进行最后一次本地修改：1git commit --amend 修改完了之后，要恢复之后的commit记录,如下:1234git rebase --continue ``` 这时通过git log你可以看到已经恢复了之前的历史记录。最后再提交本地到远端: git push origin remote_branch -f``` 注：GIT可以修改远端的历史提交，但是所修改commit及之后的所有的提交时间均被修改为修改时间了。另：如果这个过程中有操作错误，可以使用 git rebase –abort来撤销修改，回到没有开始操作合并之前的状态。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[GIT:重新组织本地提交]]></title>
      <url>http://www.waverley.me/2017/01/16/GIT-%E9%87%8D%E6%96%B0%E7%BB%84%E7%BB%87%E6%9C%AC%E5%9C%B0%E6%8F%90%E4%BA%A4/</url>
      <content type="text"><![CDATA[git可以让你对本地commit进行顺序调整，优化commit的组织性，然后再进行提交。 以git为首的SVCS现在已经很流行了，目前越来的企业和开源组织都在使用或者迁移到git上，github让git更加出色和流行。每次提到git，都会和SVN进行对比，当然这其中可以对比的因素。我这次主要就可以git可以本地提交的特性讲述如何在push之前对本地的commit进行重组。 本地commit重组的必要性在开发过程中，有很多优秀的实践指导着我们，其中有一条是“每次提交一个功能点”，意思是说每次提交应该是一个独立的单元，还有另外一条优秀实践是“频繁提交，小提交”，意思是说要频繁提交代码，并且最好是小的提交。其实这两条都没错，但是总是感觉有点冲突和矛盾。如果想完美做到第一条，那么写代码之前要好好规划，每次提交要修改哪些文件，然后才能一次提交一个完整的单元，但是这么做又会产生大commit，和第二条有冲突。考虑到很难在开发之前，设计那么清楚，就像设计大功能一样，可以边做边重构，甚至是做完重构。 所以针对git本地的commit也是一样，可以先比较频繁地执行小提交，这样每个小提交可以达到一个提交一个功能了。这样本地会产生很多commit，这些commit比较散乱，逻辑关联性也不强，甚至有些commit是本地测试用的，有的commit需要拆成两个，有个多个commit可以合并，commit之间顺序需要调换等等，经过这些过程，本地commit可以调整得很有序，而且组织性特别好。 案例需要开发三个功能function-1、function-2以及function-3，由于自己没有想清楚，开发有点散乱，提交之后的git log如下：1234568e68ded function-3-all-bigaad65ef function-2[2/2]721d5e9 function-1[2/2]8ac73c9 function-2[1/2]f1927eb modified properties to local testb29c96f function-1[1/2] 从log可以看到，function-1被分在了两个commit提交，而且中间夹杂着一个为了测试修改配置的commit（这个commit不应该push到代码库），仔细去看，function-1和function-2穿插着提交了。而function-3是一个比较大的提交，提交之后我觉得可以function-3完全可以拆成两个子功能分开提交。 如何实现一、先把function-1和function-2的穿插问题解决掉，就是调换function-2[1/2]和function-1[2/2]顺序，因为是两个功能，所以代码不会很大的冲突。执行git rebase -i进入交互模式，自动打开vim，内容如下：123456pick b29c96f function-1[1/2]pick f1927eb modified properties to local testpick 8ac73c9 function-2[1/2]pick 721d5e9 function-1[2/2]pick aad65ef function-2[2/2]pick ac10187 function-3-all-big 从这个看上去和git log的输出很相似，只是顺序恰好是倒置的，最先提交的commit在最上边。现在调换function-2[1/2]和function-1[2/2]顺序。123456pick b29c96f function-1[1/2]pick f1927eb modified properties to local testpick 721d5e9 function-1[2/2]pick 8ac73c9 function-2[1/2]pick aad65ef function-2[2/2]pick ac10187 function-3-all-big 保存，退出编辑器，git会自动执行rebase操作，之后执行git log观察下输出：123456b761ac1 function-3-all-big4152266 function-2[2/2]9c09ceb function-2[1/2]c9fed87 function-1[2/2]f1927eb modified properties to local testb29c96f function-1[1/2] 发现function-2[1/2]和function-1[2/2]顺序已经调换了。 二、删除modified properties to local test这个commit，使得本地测试代码不会push到远程代码库。这个比较容易，执行git rebase -i，在编辑器中直接删除pick f1927eb modified properties to local test这一行，保存并退出编辑器，执行rebase操作。此后可以执行git log看下输出内容：12345998a582 function-3-all-bigb67b728 function-2[2/2]ff9f827 function-2[1/2]1a7ebaa function-1[2/2]b29c96f function-1[1/2] 不需要push的那个提交已经被踢出了。 三、把需要合并的提交合并掉使其变成一个更内聚的提交。首先合并function-1[1/2]和function-1[2/2]，执行git rebase -i，输出如下：12345pick b29c96f function-1[1/2]pick 1a7ebaa function-1[2/2]pick ff9f827 function-2[1/2]pick b67b728 function-2[2/2]pick 998a582 function-3-all-big 这个只需要把第二行的pick改成s，保存退出编辑器，这个时候会在编辑器中重新编辑前面两个commit的comment，于是修改正function-1即可。看下git log输出：1234ac884c8 function-3-all-bigb992119 function-2[2/2]9549400 function-2[1/2]150ff1a function-1 这样看来function-1的两部分被合并了，变成一个单一的commit了。同样的方式来处理function2的两个commit。最后的git log输出：123236128f function-3-all-big1188917 function-2150ff1a function-1 function-1和function-2都合并了。 四、拆分function-3为function-3-module-1和function-3-module-2两个独立commit。先执行git reset –soft HEAD^，这样先回退一个commit，变成function-3提交前的状态，这样可以commit的内容还在，只是出于未提交状态。这样就可以自由选择把未提交的内容分成几次提交了，我们做成两个commit。现在git log看下结果：12343a27021 function-3-module-27829bd8 function-3-module-11188917 function-2150ff1a function-1 五、结果已经完美了，执行git push，当然先执行git pull –rebase可能是个更好的习惯。 总结因为git有local commit，既可以“随意”提交小提交，又可以在push之前修整成很漂亮的功能单元，一个commit一个单元，从而让我们有“后悔药”，也是为了做出更好的代码质量。不过一定要注意，rebase应该只操作还未push到远程仓库的commit，一旦push到了远程仓库，那么不允许再修改commit，不然会给其他开发带来很多麻烦。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python高并发爬取网络数据]]></title>
      <url>http://www.waverley.me/2016/12/02/python%E9%AB%98%E5%B9%B6%E5%8F%91%E7%88%AC%E5%8F%96%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE/</url>
      <content type="text"><![CDATA[对比测试协程（asyncio）和异步IO（aiohttp）在爬虫中的优秀表现。 Python中解决IO密集型任务（打开多个网站）的方式有很多种，比如多进程、多线程。但理论上一台电脑中的线程数、进程数是有限的，而且进程、线程之间的切换也比较浪费时间。所以就出现了“协程”的概念。本文将利用协程（asyncio）和异步IO（aiohttp）快速爬取网络数据，并对三种获取方式进行对比。 一、Requests方式Requests 是用Python语言编写，基于 urllib，采用 Apache2 Licensed 开源协议的 HTTP 库。它比 urllib 更加方便，可以节约我们大量的工作。下面我们使用Requests来获取雪球网站3000只股票信息（这里我用使用uid和token来登录网站网站）：1234567891011121314151617181920212223# coding:utf-8import requestsimport jsonimport timeheaders = &#123; &apos;Cookie&apos;: &apos;xq_a_token=XXX;u=XXX&apos;, &apos;User-Agent&apos;: &apos;Xueqiu Android 8.9&apos;, &apos;Host&apos;: &apos;stock.xueqiu.com&apos;, &apos;Pragma&apos;: &apos;no-cache&apos;, &apos;Connection&apos;: &apos;keep-alive&apos;, &apos;Accept-Encoding&apos;: &apos;gzip&apos;, &apos;Accept-Language&apos;: &apos;zh-CN,zh;q=0.8&apos;&#125;session = requests.Session()session.headers.update(headers)url = &apos;https://xueqiu.com/stock/forchartk/stocklist.json?symbol=SH601211&amp;period=60m&amp;type=normal&amp;begin=1480986000000&amp;end=&amp;_=1479483981359&apos;while True: start_time = time.time() for _ in range(300):#重复300次 request = session.get(url) stocks = json.loads(request.text) end_time = time.time() print(&quot;%s second&quot; % (end_time - start_time)) 从运行结果可以看到获取300只股票大概需要12s，那么3000只大概是120s；我们同时也可以看到程序的网络带宽利用率最大值仅仅为650kBit/s; 二、threadpool和requests这次我们使用threadpool来增加并发能力，提升获取的速率；123456789101112131415161718192021222324252627282930313233# coding:utf-8import timeimport jsonimport requestsimport threadpoolheaders = &#123; &apos;Cookie&apos;: &apos;xq_a_token=xxx;u=xxx&apos;, &apos;User-Agent&apos;: &apos;Xueqiu Android 8.8&apos;, &apos;Host&apos;: &apos;stock.xueqiu.com&apos;, &apos;Pragma&apos;: &apos;no-cache&apos;, &apos;Connection&apos;: &apos;keep-alive&apos;, &apos;Accept-Encoding&apos;: &apos;gzip&apos;, &apos;Accept-Language&apos;: &apos;zh-CN,zh;q=0.8&apos;&#125;url = &apos;https://xueqiu.com/stock/forchartk/stocklist.json?symbol=SH601211&amp;period=60m&amp;type=normal&amp;begin=1480986000000&amp;end=&amp;_=1479483981359&apos;session = requests.Session()session.headers.update(headers)def get_stocks(url_list): request = session.get(url_list) stocks = json.loads(request.text) return stocks#模拟3000只股票url_list = [url for x in range(3000)]pool = threadpool.ThreadPool(70)while True: start_time = time.time() requests = threadpool.makeRequests(get_stocks, url_list) [pool.putRequest(req) for req in requests] pool.wait() end_time = time.time() print(&quot;%s second&quot; % (end_time - start_time)) 这次我们从运行结果可以看到，使用线程池后，获取3000只股票信息只需要25s了；并且这里的70个线程已经是最佳效果了；此时，我们可以看到程序的网络带宽利用率最大值已经达到了4.54MBit/s。 三、asyncio和aiohttp高并发现在，我们来一起测试下asyncio和aiohttp的高并发能力：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# coding:utf-8import timeimport asyncioimport jsonimport aiohttpimport yarlclass TestAIOhttp: def __init__(self): self.headers = &#123; &apos;Cookie&apos;: &apos;xq_a_token=xxx;u=xxx&apos;, &apos;User-Agent&apos;: &apos;Xueqiu Android 8.8&apos;, &apos;Host&apos;: &apos;stock.xueqiu.com&apos;, &apos;Pragma&apos;: &apos;no-cache&apos;, &apos;Connection&apos;: &apos;keep-alive&apos;, &apos;Accept-Encoding&apos;: &apos;gzip&apos;, &apos;Accept-Language&apos;: &apos;zh-CN,zh;q=0.8&apos; &#125; self.url = &apos;https://xueqiu.com/stock/forchartk/stocklist.json?period=60m&amp;type=normal&amp;begin=1480986000000&amp;end=&amp;_=1479483981359&amp;symbol=&apos; #模拟3000只股票 self.stock_list = [&apos;SH601211&apos; for x in range(3000)] @property def all(self): return self.get_stock(self.stock_list) async def get_stocks_by_range(self, params): url = yarl.URL(self.url + params, encoded=True) try: async with self.__session.get(url, timeout=10, headers=self.headers) as r: response_text = await r.text() return response_text except asyncio.TimeoutError: return None def get_stock(self, stock_list): self.__session = aiohttp.ClientSession() coroutines = [] result_str = &apos;&apos; for params in stock_list: coroutine = self.get_stocks_by_range(params) coroutines.append(coroutine) try: loop = asyncio.get_event_loop() except RuntimeError: loop = asyncio.new_event_loop() asyncio.set_event_loop(loop) res = loop.run_until_complete(asyncio.gather(*coroutines)) self.__session.close() return &apos;[&apos; + &apos;,&apos;.join([x for x in res if x is not None and len(x) &gt; 2]) + &apos;]&apos; if __name__ == &apos;__main__&apos;: q = TestAIOhttp() while True: start_time = time.time() data = q.all end_time = time.time() print(&quot;%s second&quot; % (end_time - start_time)) 这次我们获取3000只股票信息只需要10s了，同时程序的网络带宽利用率最大值也达到了将近10MBit/s，这也是我20M带宽wifi的极限了；此处的测试瓶颈已经是我的网络带宽了，若提供更多的带宽资源，得到的数据可能会更加完美；不过，这里的三种方法的对比结果已经很明显了，在需要迅速爬取大量网络数据时，asyncio和aiohttp不失是一种不错的选择。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Gmaps:轻松集成Google Maps]]></title>
      <url>http://www.waverley.me/2016/10/21/Gmaps-%E8%BD%BB%E6%9D%BE%E9%9B%86%E6%88%90Google%20Maps%E7%9A%84Jupyter%20Notebook%E6%8F%92%E4%BB%B6/</url>
      <content type="text"><![CDATA[Gmaps是一个嵌入了Google Mpas的jupyter插件，是一个强大的数据可视化工具. Gmaps可以让你很简单地调用Google Maps，本文将简单介绍Gmaps的入门知识,以及有关热力图的绘制。 一、安装 最新版本的Gmaps需要升级你的IPython到4.2及以上版本 1pip install -U jupyter 用pip命令进行模块安装，并使jupyter加载 12pip install gmapsjupyter nbextension enable --py gmaps 二、申请API Key 要通过Google Maps的认证，你首先需要申请一个API Key，申请连接。进入申请界面，创建工程，然后点击凭证，选择API密钥，如下图 获取API Key之后，通过如下函数来进行配置 1gmaps.configure(api_key=&quot;AI...&quot;) 或者，通过环境变量来进行设置（文件 ~/.profile 或 ~/.bashrc） 1export GOOGLE_API_KEY=AI... 然后，在jupyter中通过如下方式使用 123import osimport gmapsgmaps.configure(api_key=os.environ[&quot;GOOGLE_API_KEY&quot;]) 这样，Maps和图层就可以通过你的API key来进行认证； 三、开始使用Gmaps是一个嵌入了Google Mpas的jupyter插件，是一个数据可视化工具。首先，让我们来画一个地震分布图：12345678910111213import gmapsimport gmaps.datasetsgmaps.configure(api_key=&quot;AI...&quot;) # Fill in with your API keyearthquake_data = gmaps.datasets.load_dataset(&quot;earthquakes&quot;)print(earthquake_data[:4]) # first four rows#在Google Mpas上画出地震分布图m = gmaps.Map()m.add_layer(gmaps.WeightedHeatmap(data=earthquake_data))m 这是一个完整的Google Maps，你可以放大缩小、卫星视图、街景等，并且热力图将会自适应。 四、基本原理gmaps是在原生Google Maps上添加一个图层；当你创建一个基本地图后，然后你可以在地图上增加一个新的图层，例如热力图层：1234567891011import gmapsgmaps.configure(api_key=&quot;AI...&quot;)m = gmaps.Map()# 初始化一些数据data = [(51.5, 0.1), (51.7, 0.2), (51.4, -0.2), (51.49, 0.1)]heatmap_layer = gmaps.Heatmap(data=data)m.add_layer(heatmap_layer)m 你还可以通过一些参数来调整图层，如下两种方式是等价的：12heatmap_layer = gmaps.Heatmap(data=data)heatmap_layer.point_radius = 8 和1heatmap_layer = gmaps.Heatmap(data=data, point_radius=8) 第一种可在图层已经建立之后，通过调整参数来动态调整图层； 五、热力图热力图是一种很好的描述特定地理位置的某一事件集中程度的方法，是一个可以展示大量数据信息的强大工具。例如，我们将1997年到2015年之间发生在非洲的110,000 起暴力事件展示出来;12345678910import gmapsimport gmaps.datasetsgmaps.configure(&quot;AI...&quot;)m = gmaps.Map()data = gmaps.datasets.load_dataset(&quot;acled_africa.csv&quot;)heatmap_layer = gmaps.Heatmap(data=data)m.add_layer(heatmap_layer)m 缩放热散当你在放大或缩小地图时，热力点可能会消失，这时你可以用max_intensity（最大峰值强度）参数来进行设置，当你的数据有明显的峰值时，这个参数是非常有用的；这个值通常配合point_radius（点半径）参数一起来进行调整，直到调整到你以为合适的数值：12heatmap_layer.max_intensity = 100heatmap_layer.point_radius = 5 Google maps还提供了一个dissipating参数（默认为true），当为true时，每个点的影响半径与缩放级别相关；当为false时，每个点覆盖的物理半径保持不变； 设置颜色梯度和不透明度Gmaps可以通过一个颜色列表来进行颜色梯度的设置，并且Google maps将会自动在相邻颜色间进行插值，颜色设置可以这样 12345heatmap.gradient = [ &apos;white&apos;, &apos;silver&apos;, &apos;gray&apos;] 或者RGB/RGBA元组12345heatmap.gradient = [ (200, 200, 200, 0.6), (100, 100, 100, 0.3), (50, 50, 50, 0.3)] 还可以通过opacity参数来调整热力图层的不透明度。 权重热力图权重热力图与热力图一样，仅仅是多了数据元组中多了权重项(latitude, longitude, weight)；由于Google Maps的限制，权重项必须是正值； 六、其他功能 Gmaps插件的功能远不止这些，以上仅仅介绍了有关热力图部分，更多功能等待大家一同去学习。项目源码见我的github。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Jupyter Notebook：交互计算工具]]></title>
      <url>http://www.waverley.me/2016/09/04/Jupyter%20Notebook%EF%BC%9A%E4%BA%A4%E4%BA%92%E8%AE%A1%E7%AE%97%E5%B7%A5%E5%85%B7/</url>
      <content type="text"><![CDATA[Jupyter Notebook，一个web界面的vim+跨平台+即时运行结果的强大online编辑器。 Jupyter Notebook（此前被称为 IPython notebook）是一个交互式笔记本，支持运行 40 多种编程语言如Python, R, Julia and Scala。Jupyter Notebook是一个基于WEB的程序，可以用于数据清洗和转换，数值模拟，统计建模，机器学习，数据科学等。 支持运行 超过40种编程语言，如Python, R, Julia and Scala 可用多种方式进行分享，如email, Dropbox, GitHub and the Jupyter Notebook Viewer 可产生丰富的输出，并可以进行可视化实时操作，如图像, 视频, LaTeX, and JavaScript 使用大数据工具Apache Spark，pandas, scikit-learn, ggplot2, dplyr等 一、安装Jupyter NotebookJupyter可以运行多种编程语言，然而Jupyter Notebook的安装依赖于Python（Python 2.7，Python 3.3或更高）。官方推荐用 Anaconda 进行Python和Jupyter的安装。1、下载相应平台的Anaconda并按指示进行安装；2、运行一下命令，启动Jupyter Notebook1jupyter notebook 对于Python用户，也可用pip进行Jupyter Notebook 的安装1、更新pip到最新版本1pip install --upgrade pip 2、使用如下命令安装1pip install jupyter 3、运行命令，启动Jupyter Notebook1jupyter notebook 运行上面的命令之后，你将看到类似下面这样的输出： 启动jupyter notebook后默认监听于本地8888，如果想外部访问也很简单jupyternotebook–ip=x.x.x.x 这样，任何知道 notebook 地址的人都可以连接到 notebook 进行远程使用。 二、使用Jupyter Notebook会在你开启 notebook 的文件夹中启动 Jupyter 主界面，如下：如果想新建一个 notebook，只需要点击New，选择你希望启动的 notebook 类型即可（这里我只安装了一个python内核）；在新打开的标签页中，我们会看到 notebook 界面，目前里面什么也没有：下方截图中看到的是一个代码单元格（code cell），以[ ]开头。在这种类型的单元格中，可以输入任意代码并执行，类似一个在线的Python交互界面。例如，输入1 + 2并按下Shift + Enter；之后，单元格中的代码就会被计算，光标也会被移动动一个新的单元格中，你会得到如下结果：再次输入：1print(&quot;Hello Jupyter!&quot;) 得到下图结果，但这次没有出现类似Out2这样的文字。这是因为我们将结果打印出来了，没有返回任何的值： notebook 有一个非常有趣的特性，就是可以修改之前的单元格，对其重新计算，这样就可以更新整个文档了。试着把光标移回第一个单元格，并将1 + 2修改成2 + 3，然后按下Shift + Enter重新计算该单元格。你会发现结果马上就更新成了 5。如果你不想重新运行整个脚本，只想用不同的参数测试某个程式的话，这个特性显得尤其强大。不过，你也可以重新计算整个 notebook，只要点击Cell -&gt; Run all即可。 Jupyter Notebook的功能显然不止这些，还支持Markdown、LaTex 语法等，可用来编辑在线文档； 三、导出功能Jupyter Notebook还有一个强大的特性，就是其导出功能。可以将 notebook 导出为多种格式： HTML Markdown ReST PDF（通过 LaTeX） Raw Python 导出 PDF 功能，可以让你不用写 LaTex 即可创建漂亮的 PDF 文档。你还可以将 notebook 作为网页发布在你的网站上。甚至，你可以导出为 ReST 格式，作为软件库的文档。 四、Notebook WidgetsJupyter Notebook集成了多种部件，如gmaps，Matplotlib等Gmaps是一个数据可视化工具，是Jupyter Notebook的一个嵌入了Google Maps的插件。敬请关注《Gmaps:轻松集成Google Maps的Jupyter Notebook插件》先贴一张图(地震分部热力图)：]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Git入门手册]]></title>
      <url>http://www.waverley.me/2016/04/16/Git%E5%85%A5%E9%97%A8%E6%89%8B%E5%86%8C/</url>
      <content type="text"><![CDATA[同生活中的许多伟大事件一样，Git 诞生于一个极富纷争大举创新的年代。 Git是一个开源的分布式版本控制系统，可以有效、高速的处理从很小到非常大的项目版本管理。Git是目前世界上最先进的分布式版本控制系统（没有之一），将Git有关命令记录如下。 安装git 1sudo apt-get install git 配置git信息12git config --global user.name &quot;waverley&quot;git config --global user.email &quot;XXX&quot; 初始化一个项目,所有文件都在目录waverley之下12345678#你可以计划改动(把它们添加到缓存区),使用如下命令:git add waverleygit add *git rm filegit rm -r dir#这是 git 基本工作流程的第一步;使用如下命令以实际提交改动:git commit -m &quot;代码提交信息&quot;#现在,你的改动已经提交到了 HEAD,但是还没到你的远端仓库。 检查都做了哪些修改123git diff #只能查到git add之前的修改文件git diff --cached #可以查到git commit之前的文件git status #查看git commit之前都有哪些文件发生了改动 查看提交日志12git loggit log -p #可以查看详细的日志 查看操作记录1git reflog 创建并管理分支12345git branch waverley #创建一个叫waverley的分支git branch #显示当前都有哪些分支,其中标注*为当前所在分支git checkout waverley #将当前分支转移到 waverley 分支git commit -a #提交分支git merge waverley #将waverley合并到主分支 克隆一个远程仓库到本地1git clone remote_path local_path 推送改动12345#你的改动现在已经在本地仓库的 HEAD中了。执行如下命令以将这些改动提交到远端仓库:git push origin master#可以把 master 换成你想要推送的任何分支。如果你还没有克隆现有仓库,并欲将你的仓库连接到某个远程服务器,你可以使用如下命令添加:git remote add origin &lt;server&gt;;#如此你就能够将你的改动推送到所添加的服务器上去了。 分支分支是用来将特性开发绝缘开来的。在你创建仓库的时候,master是“默认的”。在其他分支上进行开发,完成后再将它们合并到主分支上。创建一个叫做“feature_x”的分支,并切换过去:1git checkout -b feature_x 切换回主分支：1git checkout master 再把新建的分支删掉1git branch -d feature_x 除非你将分支推送到远端仓库，不然该分支就是不为他人所见的：1git push origin 更新与合并要更新你的本地仓库至最新改动，执行：1git pull 相当于在你的工作目录中，获取(fetch) 并 合并(merge) 远端的改动。要合并其他分支到你的当前分支(例如 master),执行:1git merge 这两种情况下,git 都会尝试去自动合并改动。不幸的是,自动合并并非次次都能成功,并可能导致冲突(conflicts)。 这时候就需要你修改这些文件来人肉合并这些冲突了。改完之后,你需要执行如下命令以将它们标记为合并成功:1git add 在合并改动之前,也可以使用如下命令查看:1git diff 标签在软件发布时创建标签,是被推荐的。这是个旧有概念,在SVN中也有。可以执行如下命令以创建一个叫做 1.0.0 的标签:1git tag 1.0.0 1b2e1d63ff 1b2e1d63ff 是你想要标记的提交 ID 的前 10 位字符。使用如下命令获取提交 ID:1git log 你也可以用该提交 ID 的少一些的前几位,只要它是唯一的。 回退回退本地修改假如你做错事(自然,这是不可能的),你可以使用如下命令替换掉本地改动:1git checkout -- 此命令会使用 HEAD 中的最新内容替换掉你的工作目录中的文件;已添加到缓存区的改动,以及新文件,都不受影响。 回退远端修改假如你想要丢弃你所有的本地改动与提交,可以到服务器上获取最新的版本并将你本地主分支指向到它:12git fetch origingit reset --hard origin/master 总结git是一个很好的版本管理工具，将常用命令总结如下图：]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hello World]]></title>
      <url>http://www.waverley.me/2016/03/25/hello-world/</url>
      <content type="text"><![CDATA[Hello World 花了两天时间搭建了个博客，也算是有了一个属于自己的私有空间了。hexo+github+coding+markdown的方式使用起来也很方便，还申请了自己的域名waverley.me。 现在，我终于可以大声说：Hello World !]]></content>
    </entry>

    
  
  
</search>
